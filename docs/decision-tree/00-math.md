# 决策树回归数学基础（CART）

这一章先讲**决策树回归**的原理与核心公式，便于后续理解代码。

---

## 1. 决策树回归是什么

- 决策树回归是一种**非参数**模型，能拟合**非线性关系**。
- 它通过一系列“如果…那么…”的规则，把数据分成多个区域。
- 每个叶子节点输出一个数值（通常是该区域样本的平均值）。

---

## 2. 树的结构

- **根节点**：所有样本的起点
- **内部节点**：对某个特征进行切分（如 `MedInc <= 3.2`）
- **叶子节点**：给出预测值（回归问题中一般是平均值）

---

## 3. 回归树的切分准则

回归树的目标：让切分后的“子节点更纯”。

最常用的指标是**均方误差（MSE）**或**方差**。

### 3.1 节点内的误差

假设一个节点包含样本集合 $R$，回归树在该节点的预测是均值：

$$
\hat{y}_R = \frac{1}{|R|} \sum_{i \in R} y_i
$$

节点误差（MSE）：

$$
\text{MSE}(R) = \frac{1}{|R|} \sum_{i \in R} (y_i - \hat{y}_R)^2
$$

### 3.2 切分增益

一个切分将节点分成左子集 $R_L$ 和右子集 $R_R$：

目标是最小化：

$$
\text{MSE}(R_L) + \text{MSE}(R_R)
$$

也可以理解为：

> 选择能让误差下降最多的切分。

---

## 4. 贪心策略（CART）

决策树在每一步都采用**贪心**策略：

1) 在所有特征上尝试所有可能切分
2) 选择让误差下降最多的切分
3) 递归继续

优点：简单高效
缺点：可能不是全局最优

---

## 5. 叶子节点的预测

回归树在叶子节点的预测值通常为：

$$
\hat{y} = \frac{1}{n_{leaf}} \sum y_i
$$

即：**叶子中样本的平均值**。

---

## 6. 过拟合与控制

决策树容易过拟合，因此需要控制复杂度：

- `max_depth`：最大深度
- `min_samples_split`：节点继续分裂的最少样本数
- `min_samples_leaf`：叶子节点最少样本数
- `max_features`：每次分裂考虑的特征数

这些参数越小，模型越复杂，越容易过拟合。

---

## 7. 特征重要性

sklearn 提供的 `feature_importances_` 基于 **不纯度下降**：

- 某特征在各节点带来的误差降低越大，重要性越高。
- 这是一种“启发式”度量，不代表因果关系。

---

## 8. 决策树 vs 线性回归

| 对比 | 线性回归 | 决策树回归 |
|------|----------|------------|
| 关系假设 | 线性 | 可拟合非线性 |
| 特征缩放 | 通常需要 | 不需要 |
| 解释性 | 高 | 规则型可解释 |
| 过拟合 | 相对少 | 容易过拟合 |

---

## 9. 小结

- 决策树回归用**分段规则**逼近复杂函数
- 切分标准是“让子节点更纯（误差更小）”
- 需要控制深度和叶子大小避免过拟合

理解这些原理后，再看代码会更清晰。
